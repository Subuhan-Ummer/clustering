# -*- coding: utf-8 -*-
"""ML-Assignment-5-Clustering_Algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yLiZYhpINykI-ZzR7YoLh2Rn2h6_KPT-

Loading and Preprocessing
"""

import pandas as pd
from sklearn.datasets import load_iris

# Loading iris dataset
iris = load_iris()
data = pd.DataFrame(iris.data, columns=iris.feature_names)
data['Species'] = iris.target

data.head()

# Dropping species column
X = data.drop('Species', axis=1)

X_km = X.copy()

X_km.head()

"""Clustering Algorithm Implementation

###A) KMeans Clustering

* Description of KMeans Clustering :
>Kmeans is a centroid-based clustering algorithm. It partitions data into 'K' clusters, where each cluster is represented by its centroid. The algorithm minimizes the variance within clusters, iteratively adjusting centroids until convergence.

* Suitability for the Iris dataset.
> The Iris dataset is suitable for KMeans clustering because it has continuous numeric features and known separable groups (species), making it a good candidate for centroid-based partitioning.
"""

# KMeans implementation

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Finding optimal k value for implementing KMeans
inertia_values = []
k_range = range(1, 11)

for k in k_range:
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(X_km)
  inertia_values.append(kmeans.inertia_)


plt.plot(k_range, inertia_values, marker='o')
plt.title('Elbow-method')
plt.xlabel('K-Values')
plt.ylabel('Inertia-Values')

plt.show()

from sklearn.metrics import silhouette_score, davies_bouldin_score

# Range of k values to evaluate
optimal_k_range = range(3, 6)

for k in optimal_k_range:
  kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
  cluster_labels = kmeans.fit_predict(X_km)

  print(f'Optimal k values as {k}')
  print('------------------------------')

  # Calculating Silhouette Score
  sil_score = silhouette_score(X_km, cluster_labels)
  print(f'Silhouette Score : {sil_score:.2f}')

  # Calculating Davies-Bouldin Score
  db_score = davies_bouldin_score(X_km, cluster_labels)
  print(f'Davies_Bouldin Score : {db_score:.2f}')
  print('\n')

"""##Based on the metrics:

###Optimal k = 3

* It has the highest Silhouette Score (0.55)
* It has the lowest Davies-Bouldin Score (0.66)
"""

# Choosing optimal number of clusters as 3 from elbow plot and metrics

optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
X_km['KMeans_Cluster'] = kmeans.fit_predict(X_km)

X_km.head()

# Evaluate model performance using original features
original_features_km = X_km.drop('KMeans_Cluster', axis=1)

# Silhouette score
sil_score = silhouette_score(original_features_km, X_km['KMeans_Cluster'])
print(f'Silhouette Score: {sil_score:.2f}')

# Davies-Bouldin score
db_score = davies_bouldin_score(original_features_km, X_km['KMeans_Cluster'])
print(f'Davies-Bouldin Score: {db_score:.2f}')

# Visualisation of Clusters using PCA and t-SNE
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Dimensionality reduction with PCA
pca = PCA(0.95)
X_km_pca = pca.fit_transform(original_features_km)

# Visualizing PCA results
plt.figure(figsize=(10, 5))
plt.subplot(1,2,1)
scatter = plt.scatter(X_km_pca[:, 0], X_km_pca[:, 1], c=X_km['KMeans_Cluster'], cmap='Paired', s=10)
plt.colorbar(scatter, label='Classes')
plt.title("PCA visualization")
plt.xlabel('Principal component 1')
plt.ylabel('Principal component 2')

# Dimensionality reduction with t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)
X_km_tsne = tsne.fit_transform(original_features_km)

# Visualizing t-SNE results
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_km_tsne[:, 0], X_km_tsne[:, 1], c=X_km['KMeans_Cluster'], cmap='Paired', s=10)
plt.colorbar(scatter, label='Classes')
plt.title('t-SNE visualization')
plt.xlabel('t-SNE component 1')
plt.ylabel('t-SNE component 2')

plt.tight_layout()
plt.show()

"""###B) Hierarchical Clustering

* Description of Hierarchical Clustering:
> Hierarchical clustering buiulds a tree (dendrogram) representing nested clusters. The algorithm can be divisive (top-down) or agglomerative (bottom-up). The clusters are formed by grouping data points based on their similarity.

* Suitability for the Iris dataset:
> Hierarchical clustering is suitable for the Iris dataset because it provides a detailed view of how data points cluster at different levels, which can reveal relationships between clusters.
"""

# Implementation of Hierarchical Clustering

import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering

X_hc = X.copy()

X_hc.head()

# Dendrogram
plt.figure(figsize=(10, 7))
plt.title('Dendrogram')
dend = shc.dendrogram(shc.linkage(X_hc, method='ward'))
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()

# Unique colors for optimal number of clusters
unique_colors = set(dend['color_list'])

unique_colors

# Optimal number of clusters
optimal_number_of_clusters = len(unique_colors) - 1

# Implementing Hierarchical Clustering
hc = AgglomerativeClustering(n_clusters=optimal_number_of_clusters, metric='euclidean', linkage='ward')
X_hc['HC_Cluster'] = hc.fit_predict(X_hc)

X_hc.head()

# Evaluate model performance using original features
original_features_hc = X_hc.drop('HC_Cluster', axis=1)

# Silhouette score
sil_score = silhouette_score(original_features_hc, X_hc['HC_Cluster'])
print(f'Silhouette Score: {sil_score:.2f}')

# Davies-Bouldin score
db_score = davies_bouldin_score(original_features_hc, X_hc['HC_Cluster'])
print(f'Davies-Bouldin Score: {db_score:.2f}')

# Dimensionality reduction with PCA
pca = PCA(0.95)
X_hc_pca = pca.fit_transform(original_features_hc)

# Visualizing PCA results
plt.figure(figsize=(10, 5))
plt.subplot(1,2,1)
scatter = plt.scatter(X_hc_pca[:, 0], X_hc_pca[:, 1], c=X_hc['HC_Cluster'], cmap='Paired', s=10)
plt.colorbar(scatter, label='Classes')
plt.title("PCA visualization")
plt.xlabel('Principal component 1')
plt.ylabel('Principal component 2')

# Dimensionality reduction with t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)
X_hc_tsne = tsne.fit_transform(original_features_hc)

# Visualizing t-SNE results
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_hc_tsne[:, 0], X_hc_tsne[:, 1], c=X_hc['HC_Cluster'], cmap='Paired', s=10)
plt.colorbar(scatter, label='Classes')
plt.title('t-SNE visualization')
plt.xlabel('t-SNE component 1')
plt.ylabel('t-SNE component 2')

plt.tight_layout()
plt.show()

